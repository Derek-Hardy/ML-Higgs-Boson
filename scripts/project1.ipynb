{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* y = Prediction (1, -1)\n",
    "* tX = values of 30 features\n",
    "* ids = unique id of each event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Backgound\n",
    "\n",
    "The Higgs boson is an elementary particle in the Standard Model of physics which explains why other particles have mass. Its discovery at the Large Hadron Collider at CERN was announced in March 2013. In this project, you will apply machine learning techniques to actual CERN particle accelerator data to recreate the process of “discovering” the Higgs particle. For some background, physicists at CERN smash protons into one another at high speeds to generate even smaller particles as by-products of the collisions. Rarely, these collisions can produce a Higgs boson. Since the Higgs boson decays rapidly into other particles, scientists don’t observe it directly, but rather measure its“decay signature”, or the products that result from its decay process. Since many decay signatures look similar, it is our job to estimate the likelihood that a given event’s signature was the result of a Higgs boson (signal) or some other process/particle (background). In practice, this means that you will be given a vector of features representing the decay signature of a collision event, and asked to predict whether this event was signal (a Higgs boson) or background (something else). To do this, you will use the binary classification techniques we have discussed in the lectures.\n",
    "\n",
    "Further Information: https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are:\n",
    "- 250, 000 data points\n",
    "- 30 different sets of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) (250000, 30) (250000,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape, tX.shape, ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = ['Id', 'Prediction', 'DER_mass_MMC', 'DER_mass_transverse_met_lep',\n",
    "       'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet',\n",
    "       'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep',\n",
    "       'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau',\n",
    "       'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt',\n",
    "       'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta',\n",
    "       'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet',\n",
    "       'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta',\n",
    "       'PRI_jet_leading_phi', 'PRI_jet_subleading_pt',\n",
    "       'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi',\n",
    "       'PRI_jet_all_pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = tX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets see how the data (x, y) looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topN(table, N):\n",
    "    top_ten = []\n",
    "    for i in range(0, N):\n",
    "        top_ten.append(table[i])\n",
    "    print(np.array(top_ten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.38470e+02  5.16550e+01  9.78270e+01  2.79800e+01  9.10000e-01\n",
      "   1.24711e+02  2.66600e+00  3.06400e+00  4.19280e+01  1.97760e+02\n",
      "   1.58200e+00  1.39600e+00  2.00000e-01  3.26380e+01  1.01700e+00\n",
      "   3.81000e-01  5.16260e+01  2.27300e+00 -2.41400e+00  1.68240e+01\n",
      "  -2.77000e-01  2.58733e+02  2.00000e+00  6.74350e+01  2.15000e+00\n",
      "   4.44000e-01  4.60620e+01  1.24000e+00 -2.47500e+00  1.13497e+02]\n",
      " [ 1.60937e+02  6.87680e+01  1.03235e+02  4.81460e+01 -9.99000e+02\n",
      "  -9.99000e+02 -9.99000e+02  3.47300e+00  2.07800e+00  1.25157e+02\n",
      "   8.79000e-01  1.41400e+00 -9.99000e+02  4.20140e+01  2.03900e+00\n",
      "  -3.01100e+00  3.69180e+01  5.01000e-01  1.03000e-01  4.47040e+01\n",
      "  -1.91600e+00  1.64546e+02  1.00000e+00  4.62260e+01  7.25000e-01\n",
      "   1.15800e+00 -9.99000e+02 -9.99000e+02 -9.99000e+02  4.62260e+01]\n",
      " [-9.99000e+02  1.62172e+02  1.25953e+02  3.56350e+01 -9.99000e+02\n",
      "  -9.99000e+02 -9.99000e+02  3.14800e+00  9.33600e+00  1.97814e+02\n",
      "   3.77600e+00  1.41400e+00 -9.99000e+02  3.21540e+01 -7.05000e-01\n",
      "  -2.09300e+00  1.21409e+02 -9.53000e-01  1.05200e+00  5.42830e+01\n",
      "  -2.18600e+00  2.60414e+02  1.00000e+00  4.42510e+01  2.05300e+00\n",
      "  -2.02800e+00 -9.99000e+02 -9.99000e+02 -9.99000e+02  4.42510e+01]\n",
      " [ 1.43905e+02  8.14170e+01  8.09430e+01  4.14000e-01 -9.99000e+02\n",
      "  -9.99000e+02 -9.99000e+02  3.31000e+00  4.14000e-01  7.59680e+01\n",
      "   2.35400e+00 -1.28500e+00 -9.99000e+02  2.26470e+01 -1.65500e+00\n",
      "   1.00000e-02  5.33210e+01 -5.22000e-01 -3.10000e+00  3.10820e+01\n",
      "   6.00000e-02  8.60620e+01  0.00000e+00 -9.99000e+02 -9.99000e+02\n",
      "  -9.99000e+02 -9.99000e+02 -9.99000e+02 -9.99000e+02  0.00000e+00]\n",
      " [ 1.75864e+02  1.69150e+01  1.34805e+02  1.64050e+01 -9.99000e+02\n",
      "  -9.99000e+02 -9.99000e+02  3.89100e+00  1.64050e+01  5.79830e+01\n",
      "   1.05600e+00 -1.38500e+00 -9.99000e+02  2.82090e+01 -2.19700e+00\n",
      "  -2.23100e+00  2.97740e+01  7.98000e-01  1.56900e+00  2.72300e+00\n",
      "  -8.71000e-01  5.31310e+01  0.00000e+00 -9.99000e+02 -9.99000e+02\n",
      "  -9.99000e+02 -9.99000e+02 -9.99000e+02 -9.99000e+02  0.00000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print_topN(tX, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-1.0: 164333, 1.0: 85667})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "freq = collections.Counter()\n",
    "for x in y:\n",
    "    freq[x] += 1\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 181,886 rows out the 250,000 have undefined values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181886\n"
     ]
    }
   ],
   "source": [
    "undefined_count = 0\n",
    "for row in tX:\n",
    "    for v in row:\n",
    "        if v == -999.0:\n",
    "            undefined_count+=1\n",
    "            break\n",
    "print(undefined_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undefined values for each feature: \n",
      "\n",
      "DER_mass_MMC : 38114 / 211886\n",
      "DER_mass_transverse_met_lep : 0 / 250000\n",
      "DER_mass_vis : 0 / 250000\n",
      "DER_pt_h : 0 / 250000\n",
      "DER_deltaeta_jet_jet : 177457 / 72543\n",
      "DER_mass_jet_jet : 177457 / 72543\n",
      "DER_prodeta_jet_jet : 177457 / 72543\n",
      "DER_deltar_tau_lep : 0 / 250000\n",
      "DER_pt_tot : 0 / 250000\n",
      "DER_sum_pt : 0 / 250000\n",
      "DER_pt_ratio_lep_tau : 0 / 250000\n",
      "DER_met_phi_centrality : 0 / 250000\n",
      "DER_lep_eta_centrality : 177457 / 72543\n",
      "PRI_tau_pt : 0 / 250000\n",
      "PRI_tau_eta : 0 / 250000\n",
      "PRI_tau_phi : 0 / 250000\n",
      "PRI_lep_pt : 0 / 250000\n",
      "PRI_lep_eta : 0 / 250000\n",
      "PRI_lep_phi : 0 / 250000\n",
      "PRI_met : 0 / 250000\n",
      "PRI_met_phi : 0 / 250000\n",
      "PRI_met_sumet : 0 / 250000\n",
      "PRI_jet_num : 0 / 250000\n",
      "PRI_jet_leading_pt : 99913 / 150087\n",
      "PRI_jet_leading_eta : 99913 / 150087\n",
      "PRI_jet_leading_phi : 99913 / 150087\n",
      "PRI_jet_subleading_pt : 177457 / 72543\n",
      "PRI_jet_subleading_eta : 177457 / 72543\n",
      "PRI_jet_subleading_phi : 177457 / 72543\n",
      "PRI_jet_all_pt : 0 / 250000\n"
     ]
    }
   ],
   "source": [
    "undefined_st = {}\n",
    "for i in range(0, M): undefined_st[i] = 0\n",
    "for row in tX:\n",
    "    for i in range(0, M):\n",
    "        if row[i] == -999.0:\n",
    "            undefined_st[i] += 1\n",
    "\n",
    "# TODO: Why not maka nice graph for below :)\n",
    "print(\"Undefined values for each feature: \\n\")\n",
    "for i in range(0, M):\n",
    "    print(\"{0} : {1} / {2}\".format(HEADERS[i+2], undefined_st[i], N - undefined_st[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_funcs import *      # MSE, gradient descent, build_poly etc...\n",
    "from implementations import *   # implemented regression algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Think about how to deal with these (-999) values. Some ideas:\n",
    "\n",
    "- Drop the row \n",
    "- Drop the feature away\n",
    "- Fill them with the average\n",
    "- Predict a reasonable value for them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(tx):\n",
    "    '''\n",
    "    Set outliers with threshold 990 to 0 in the data set.\n",
    "    '''\n",
    "    x_clean = np.c_[np.ones(tx.shape[0]), tx]\n",
    "    \n",
    "    for i in range(x_clean.shape[0]):\n",
    "        for j in range(x_clean.shape[1]):\n",
    "            # filter out outlier with 0\n",
    "            if x_clean[i, j] <= -990:\n",
    "                x_clean[i, j] = 0\n",
    "    return x_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred, actual):\n",
    "    return np.sum(pred == actual) / len(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Engineer new features from the existing set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Find out how to measure the importance of each feature? e.g. a heatmap of how correlated the features are to the y value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Least square method with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, tx, k, k_indices):\n",
    "    # Get the kth test set\n",
    "    k_test = k_indices[k]\n",
    "    \n",
    "    # Collect remaining data as training set & flatten the array\n",
    "    mask = np.ones(len(k_indices), dtype=bool)\n",
    "    mask[[k]] = False\n",
    "    k_train = k_indices[mask].ravel()\n",
    "    \n",
    "    # Split the data set\n",
    "    train_set_x = tx[k_train, :]\n",
    "    test_set_x = tx[k_test, :]\n",
    "    \n",
    "    train_set_y = y[k_train]\n",
    "    test_set_y = y[k_test]\n",
    "    \n",
    "    train_set_x = cleanData(train_set_x)\n",
    "    test_set_x = cleanData(test_set_x)\n",
    "    \n",
    "    # Calculate weight and loss using least square method\n",
    "    w, loss = least_squares(train_set_y, train_set_x)\n",
    "    \n",
    "    # Predict for the output\n",
    "    train_pred_y = predict_labels(w, train_set_x)\n",
    "    test_pred_y = predict_labels(w, test_set_x)\n",
    "    \n",
    "    # Calculate prediction accuracy\n",
    "    train_acc = compute_accuracy(train_pred_y, train_set_y)\n",
    "    test_acc = compute_accuracy(test_pred_y, test_set_y)\n",
    "    \n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Train Accuracy: 0.746667 || Test Accuracy: 0.747280\n",
      "1 - Train Accuracy: 0.746725 || Test Accuracy: 0.745008\n",
      "2 - Train Accuracy: 0.745899 || Test Accuracy: 0.748384\n",
      "3 - Train Accuracy: 0.746581 || Test Accuracy: 0.745376\n",
      "Average test accuracy: 0.746512\n",
      "Variance of test accuracy: 0.000002\n"
     ]
    }
   ],
   "source": [
    "k_fold = 4\n",
    "seed = 1\n",
    "\n",
    "# split training data into k folds\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "results_train = []\n",
    "results_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    result_train, result_test = cross_validation(y, tX, k, k_indices)\n",
    "    \n",
    "    results_train.append(result_train)\n",
    "    results_test.append(result_test)\n",
    "    \n",
    "for i in range(k_fold):\n",
    "    print(\"{ind} - Train Accuracy: {a:.6f} || Test Accuracy: {b:.6f}\".format(\n",
    "           ind=i, a=results_train[i], b=results_test[i]))\n",
    "    \n",
    "print(\"Average test accuracy: {ta:.6f}\".format(ta=np.mean(results_test)))\n",
    "print(\"Variance of test accuracy: {tv:.6f}\".format(tv=np.var(results_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ridge regression with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_ridge_regression(y, tx, k, k_indices, degree, lambda_):\n",
    "    # Get the kth test set\n",
    "    k_test = k_indices[k]\n",
    "    \n",
    "    # Collect remaining data as training set & flatten the array\n",
    "    mask = np.ones(len(k_indices), dtype=bool)\n",
    "    mask[[k]] = False\n",
    "    k_train = k_indices[mask].ravel()\n",
    "    \n",
    "    # Split the data set\n",
    "    train_set_x = tx[k_train, :]\n",
    "    test_set_x = tx[k_test, :]\n",
    "    \n",
    "    train_set_y = y[k_train]\n",
    "    test_set_y = y[k_test]\n",
    "    \n",
    "    # Build polynomial basis function\n",
    "    train_set_x = build_poly(train_set_x, degree)\n",
    "    test_set_x = build_poly(test_set_x, degree)\n",
    "    \n",
    "    # Calculate weight using ridge regression method\n",
    "    w, loss = ridge_regression(train_set_y, train_set_x, lambda_)\n",
    "    \n",
    "    # Predict for the output\n",
    "    train_pred_y = predict_labels(w, train_set_x)\n",
    "    test_pred_y = predict_labels(w, test_set_x)\n",
    "    \n",
    "    # Calculate prediction accuracy\n",
    "    train_acc = compute_accuracy(train_pred_y, train_set_y)\n",
    "    test_acc = compute_accuracy(test_pred_y, test_set_y)\n",
    "    \n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Train Accuracy: 0.791413 || Test Accuracy: 0.789152\n",
      "1 - Train Accuracy: 0.566416 || Test Accuracy: 0.569456\n",
      "2 - Train Accuracy: 0.800475 || Test Accuracy: 0.798896\n",
      "3 - Train Accuracy: 0.582928 || Test Accuracy: 0.581184\n",
      "Average test accuracy: 0.684672\n",
      "Variance of test accuracy: 0.011987\n"
     ]
    }
   ],
   "source": [
    "k_fold = 4\n",
    "seed = 10\n",
    "\n",
    "degree = 7\n",
    "lambda_ = 0.001\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "results_train = []\n",
    "results_test = []\n",
    "\n",
    "for k in range(k_fold):\n",
    "    result_train, result_test = cv_ridge_regression(y, tX, k, k_indices, degree, lambda_)\n",
    "    \n",
    "    results_train.append(result_train)\n",
    "    results_test.append(result_test)\n",
    "    \n",
    "for i in range(k_fold):\n",
    "    print(\"{ind} - Train Accuracy: {a:.6f} || Test Accuracy: {b:.6f}\".format(\n",
    "           ind=i, a=results_train[i], b=results_test[i]))\n",
    "    \n",
    "print(\"Average test accuracy: {ta:.6f}\".format(ta=np.mean(results_test)))\n",
    "print(\"Variance of test accuracy: {tv:.6f}\".format(tv=np.var(results_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Further optimisation] We can also use logistic regression to tackle this problem. Following which, we can try and move on and try other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
